
## 现代 GPU 架构（以 NVIDIA Fermi 和 RTX 4090 为例）

###  核心思想：高度并行、计算密集

#### NVIDIA Fermi 架构

* 基本单位：Streaming Multiprocessor（SM）
* 每个 SM 中包含多个 SP（Streaming Processor）

#### 以 GeForce RTX 4090 为例

| 资源类型                          | 数量                     |
| ----------------------------- | ---------------------- |
| SM（Streaming Multiprocessors） | 128 个                  |
| 核心数（SP）                       | 每个 SM 128 个，总计 16384 个 |
| 张量核心（用于 ML）                   | 每个 SM 4 个，总计 512 个     |
| 共享内存（Shared Memory）           | 每个 SM 16MB             |
| 寄存器文件                         | 每个 SM 32MB             |
| L2 Cache                      | 总共 72MB                |

>  **扩展**：张量核心（Tensor Core）是 NVIDIA 为深度学习优化的专用硬件单元，可大幅加速矩阵乘法和卷积。

---

## CUDA 执行模型（Execution Model）

### 核心特点：

* **异构计算**：CPU（主机）与 GPU（设备）协同处理
* 程序被拆分为：

  * CPU：串行部分
  * GPU：并行部分（kernel）

#### Kernel 启动语法：

```cpp
kernelName<<<numBlocks, threadsPerBlock>>>(arguments);
```

#### 模型特性：

* **SIMT**：Single Instruction, Multiple Threads（单指令多线程）
* 与 CPU 的 SIMD 相似，但更灵活

>  **扩展**：
>
> * CUDA 程序使用 **主从模型**：主机负责数据准备与调度，设备负责密集计算。
> * 实际项目中需合理区分任务：如预处理、后处理放 CPU，中间计算放 GPU。

---

##  第 10\~13 页：向量加法的传统实现 vs CUDA 实现

###  传统串行版本：

```cpp
void vecAdd(float *A, float *B, float *C, int n) {
  for (int i = 0; i < n; ++i)
    C[i] = A[i] + B[i];
}
```

###  CUDA 主机端大致流程：

1. 分配 GPU 设备内存（cudaMalloc）
2. 主机数据拷贝到设备（cudaMemcpy）
3. 启动 kernel
4. 拷贝结果回主机
5. 释放设备内存

>  **扩展：注意点**
>
> * kernel 启动前后要有 `cudaDeviceSynchronize()`（调试时更易定位错误）
> * malloc/copy/free 的开销不可忽略，**要尽量复用数据**

---

##  第 14\~17 页：数据并行与线程组织

### 数据并行的基本思想：

* 将向量 A 和 B 划分为多个任务（加法）
* 每个线程处理一个元素：即 `C[i] = A[i] + B[i]`

### 线程组织方式：

* **Grid**：线程网格（所有线程组成）
* **Block**：网格中一个小组
* 每个线程有：

  * `threadIdx`：线程在 block 中的位置
  * `blockIdx`：block 在 grid 中的位置

>  **扩展**
>
> * `blockDim` 表示 block 的维度
> * 总线程唯一 ID 计算公式：

```cpp
int i = threadIdx.x + blockIdx.x * blockDim.x;
```

---

## 线程调度与 Warp 机制

### Warp（织布机中的“经线”）

* 每个 Warp 包含 32 个线程
* Warp 是 SM 的调度单元
* 同一个 warp 中的线程必须执行相同的指令

#### 示例：

* 一个 block 有 256 个线程 → 分为 8 个 warp
* 一个 SM 被分配 3 个 block → 24 个 warp

>  **扩展**
>
> * 如果 warp 中的线程走不同的分支（if-else），则会发生 **分支分化（Divergence）**，导致性能下降

---

## 向量加法内核实现（Kernel）

### kernel 代码

```cpp
__global__
void vecAddKernel(float *A, float *B, float *C, int n) {
  int i = threadIdx.x + blockIdx.x * blockDim.x;
  if (i < n) C[i] = A[i] + B[i];
}
```

### 启动方式（主机端）：

```cpp
int threadsPerBlock = 256;
int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
vecAddKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);
```

### 内存管理：

```cpp
cudaMalloc(&d_A, size);
cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
// 同理 d_B, d_C
```

---

## 二维图像处理示例

### kernel 示例：图像每像素乘 2.0

```cpp
__global__
void PictureKernel(float* d_Pin, float* d_Pout, int height, int width) {
  int Row = blockIdx.y * blockDim.y + threadIdx.y;
  int Col = blockIdx.x * blockDim.x + threadIdx.x;
  if (Row < height && Col < width)
    d_Pout[Row * width + Col] = 2.0 * d_Pin[Row * width + Col];
}
```

>  **扩展：适用场景**
>
> * 图像滤波、卷积操作、模板匹配等都需二维线程布局

---

## GPU 内存带宽瓶颈与优化

### 问题：

* 理论 FLOPS 很高，但受限于内存带宽
* 示例：

  * GPU 理论 1500 GFLOPS，但内存带宽仅 200 GB/s
  * 导致实际只能达到 3.33% 的峰值性能

### 解决方法：

1. 减少内存访问（共享内存、合并访问）
2. **Memory Coalescing**：线程访问连续内存，减少 DRAM 请求数
3. **DRAM Bursting**：硬件机制，自动拉取邻近字节

---

## Memory Coalescing 实践

### 合并访问条件：

```cpp
A[base + threadIdx.x] // 成员连续
```

### 不合并的访问会：

* 引发多个内存事务
* 导致带宽浪费
* 实际效率低

>  **扩展建议**
>
> * 尽量让 warp 内的线程按顺序访问内存
> * 结构体数组（SoA）比数组结构体（AoS）更容易合并访问

---

## 共享内存（Shared Memory）

### 特点：

* 每个 SM 独享一块
* 比全局内存快得多（低延迟，高带宽）
* 作用范围：block 内线程可见
* 生命周期：block 执行期间

### 声明方式：

```cpp
__shared__ float tile[TILE_WIDTH][TILE_WIDTH];
```
> **扩展：使用场景**
>
> * 矩阵乘法（tiling）
> * 图像处理（局部缓存）
> * 归约计算（Reduction）

---

## 线程同步

### 关键字：

```cpp
__syncthreads();
```

### 用法：

* 保证 block 中所有线程都执行到某一同步点后，再继续执行
* 常用于共享内存的数据写入/读取后同步

> **警告**：
>
> * 只能用于 block 内同步，不能跨 block

---

如果你还希望我将这些笔记导出为 `.md` 或 `.pdf` 文件，或者继续扩展某部分内容如矩阵乘法、共享内存优化、性能分析工具（如 Nsight Compute），随时告诉我！
