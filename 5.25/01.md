# 并行计算
- 简单定义：使用并行计算机，例如多核计算机、计算机集群或其他先进的并行/分布式计算系统，以高速解决高级计算问题
- 并行计算通过将一个大任务分成多个小任务来工作，然后将每个小任务分配给一个处理器
- 注意在计算过程中，这些小任务可能需要协调，使得并行计算更加复杂
- 并行计算机的性能通常以每秒浮点运算次数（FLOPS）来衡量
- -  例如，千兆（10^9）FLOPS，太兆（10^12）FLOPS，拍兆（10^15）FLOPS，艾克兆（10^18）FLOPS
## 单个CPU的理论峰值FLOPS可以计算
- ![alt text](image.png)
- cores：指CPU的核心数量。核心数量越多，在相同条件下，单位时间能执行的运算就可能越多。
- cycles/second：即CPU的时钟频率，代表CPU每秒能够执行的时钟周期数，反映了CPU运算速度快慢
- （floating-point operations）/cycle：表示了每个时钟周期内能够执行的浮点运算数量，体现了CPU在单个时钟周期处理浮点运算的能力。
## 需要考虑的问题
1. 如何将一个任务分成多个小任务？
2. 我们如何将任务分配给进程/线程？
3. 如果进程/线程需要共享部分结果怎么办？
4. 并行程序的性能如何？
# 多核技术
## 功耗与电压，频率的关系
- power ∝ voltage*voltage*frequency（v*v*f）
- - 表示功耗与电压的平方和频率的乘积成正比 。这是芯片功耗的基本关系公式，说明电压和频率对功耗有显著影响。
- frequency \propto voltage
- - 意味着频率和电压成正比关系，即电压升高，频率也会升高，反之亦然
- power  ∝ propto frequency^3
- - 由前两个关系推导得出，进一步强调了频率对功耗的强相关性，频率的变化会使功耗呈三次方的变化趋势。
## 单核性能与功耗变化
- 单核场景：当单核频率增加50%时，性能变为原来的1.5倍（因为性能和频率在单核情况下近似成正比）。但根据 power \propto frequency^{3} ，功耗变为原来的 1.5^{3}约等于3.3 倍 。这表明单核通过提升频率来提高性能时，功耗会大幅增加。
## 多核性能与功耗变化
-  双核场景：使用双核时，为了达到与单核提升频率相同的峰值性能，可将频率降低25% 。设原来频率为 f ，降低25%后变为 0.75f 。根据power  ∝ propto frequency^3 ，此时功耗变为原来的 (0.75)^{3}=0.421875 ，这里说变为0.8倍是一种近似简化表述 。说明多核架构下，通过合理分配任务到多个核心，在达到相同性能时，可以降低频率，从而大幅降低功耗
# 大数据（数据分析）
## 数据洪流
- 含义：是指现代社会中海量数据以极快的速度产生、流动和积累的现象。随着数字化技术的普及（如物联网、社交媒体、智能设备、云计算等），数据规模呈爆炸式增长，形成如同“洪水”般汹涌而来的态势。这一概念强调数据的规模、速度和复杂性已远超传统处理能力。
- 主要特征：
- 1. 数据量巨大
- 2. 高速生成
- 3. 多样性
- 4. 价值密度低
- 应对措施
- 1. 云计算与边缘计算：分布式处理降低延迟。
- 2. 数据压缩与清洗：提升存储和处理效率。

- 3. AI与机器学习：自动化分析庞杂数据。

- 4. 法规完善：如GDPR，规范数据使用。
# 深度学习
- 深度学习（DL），或深度神经网络（DNN ），是一类受人类大脑结构和功能启发的机器学习算法。
- 深度神经网络通过分析大量数据进行训练，以实现分类和预测。

- 目前，它已在实际中成功应用于各种场景，在我们日常生活中发挥着越来越重要的作用。

- 然而，其巨大成功主要得益于可用训练数据量的增加，以及更强大的计算资源，这些使我们能够训练更大、更深的神经网络。
# 并行计算机组织
**三种物理组织类型**
- 多核（Multicore）
- - 多个核心共享内存。
- - 小规模（高端产品约 100 个核心 ）。

- 集群（Cluster）
- - 多个处理单元（PE），即配备多核处理器和 / 或图形处理器（GPU）的独立计算机，通过互连网络连接。

- - 大规模，几乎所有的超级计算机都是集群。

- 图形处理器（GPU）

- - 附加加速器 - 最初专为图形处理设计。

- - 如今可作为一种更通用的 GPU，用于具有规则计算和数据模式的应用程序，尤其是机器学习领域。
# 多核
- 多个处理器，即多个核心
- 片上缓存
- 共享全局内存空间
# 计算机集群
- 多台独立计算机通过互联网连接
- 每台独立计算机都是一个拥有本地内存的多核系统
- 大多数超级计算机属于计算机集群
- 每个计算节点不仅包含多个核心，还包含图形处理器
# 现代图形处理器
- 每个GPU包含：多个流多处理器
- - 多个流处理器
- - 寄存器文件
- - 共享内存
- 常量缓存
- 纹理缓存
- 设备内存
# 互联网络
 ## 前沿超级计算机（Frontier） ：全球首台达到百亿亿次浮点运算每秒（exascale，10^{18} FLOPS ）的超级计算机 ，部署于美国橡树岭领导计算设施
- 计算节点数量 ：使用9,472个计算节点 
- 节点配置 ：每个计算节点由一颗64核CPU和4块GPU组成。因此，整台超级计算机总计有606,208个CPU核心以及37,888块GPU 。
-  互联网络 ：计算节点之间采用最先进的Slingshot网络进行互联 。Slingshot互联网络的核心是Rosetta交换机 。其默认拓扑结构为蜻蜓拓扑（Dragonfly），这是一种分层直接拓扑结构 。
- 布局结构
- - Tile阵列：采用4行8列的tile布局 ，共64个位置（编号从P0 - P63 ） 。这种规整的阵列结构有助于系统化地组织和管理网络节点
- - 端口配置：每个tile配备两个交换机端口 。端口是数据进出tile的通道，更多端口意味着更高的数据传输灵活性与带宽潜力。
- 交换机制  
- -  分布式交叉开关：基于行总线（row busses）、列通道（column channels）和每个tile内的交叉开关（per - tile crossbars）构建分布式交叉开关 。行总线和列通道提供了数据在不同行和列间传输的主干道，而每个tile内的交叉开关则负责在tile内部灵活地引导数据流向不同端口 。
## 罗塞塔交换机
- 端口配置：罗塞塔交换机有64个端口，端口速率为200 Gb/s 。这些端口可连接计算节点，也可连接其他交换机，以构成不同的互联网络 。64个端口被分组为32个tile，每个tile包含2个端口 
-  结构布局：tile排列成4行，每行8个 。同一行的tile通过16条每行总线连接 ，这些总线用于将数据从相应端口发送到该行的其他端口 。同一列的tile通过每个tile内的交叉开关连接 ，每个tile内的交叉开关有来自该行16个端口的16个输入，以及到该列8个端口的8个输出
-  传输跳数：从一个端口到另一个端口最多只需2跳（直径为2 ） ，这意味着数据在交换机内传输经过的节点较少，能有效减少延迟 。 
## 蜻蜓拓扑
### 拓扑
- 定义：用于描述网络中设备的连接和通信方式，即网络拓扑结构 。通过将网络中的计算机、服务器、交换机等设备抽象为“节点”，连接设备的传输介质（如网线、光纤等）抽象为“线”，来研究这些节点和线之间的相连关系 。
- 常见类型及特点
- - 星型：以中央节点为中心，其他节点与之相连。结构简单，便于管理和故障排查，但中央节点故障会致全网瘫痪 。
- - 环型：节点首尾相连成环，数据沿固定方向传输。控制简单，但节点过多会影响传输速率，且一处故障可能导致全网故障 。
- - 总线型：所有节点挂在一条总线上，结构简单、易扩充，但故障查找困难，单点故障可能影响全网
- - 网状型：设备间点到点连接，可靠性高、容错能力强，但成本高、安装和管理复杂 。
### 蜻蜓拓扑
-  拓扑类型：蜻蜓拓扑是一种分层直接拓扑 
-  交换机分组：交换机被组织成组，通常以机柜为单位 。 这种拓扑结构有助于在大规模网络中优化数据传输路径，提升整体网络的可扩展性和性能 。
- 组内连接：在每个组内，交换机之间通过电气链路（如铜缆 ）以全连接图的方式相连 ，即组内任意两个交换机之间都有直接连接 ，这种连接方式保证了组内通信的高效性。
- 组间连接：不同组之间同样以全连接图的方式相连，但采用的是光链路（如光缆 ） 。光链路具有高速、长距离传输损耗小等优势，适合用于组间的高速数据传输 。
# 计算机分类
**Flynn分类法**：由斯坦福大学的Michael Flynn教授在20世纪60年代提出，用于对计算机进行分类 。现代大多数计算机是这些分类的组合形式
## 分类类型
- SISD（单指令流单数据流 ）：对应单处理器系统，一次执行一条指令，处理一个数据流，早期的个人计算机多属于此类
- SIMD（单指令流多数据流 ）：包括处理器阵列、流水线向量处理器等。一条指令可同时对多个数据进行操作，常用于多媒体处理、科学计算等领域，能加速对大量相似数据的处理
- MISD（多指令流单数据流 ）：很少被使用，它试图用多条不同指令处理单个数据流，实际应用场景有限
- MIMD（多指令流多数据流 ）：涵盖多处理器、多计算机系统，可同时执行多条指令，处理多个数据流，具备高度并行处理能力，常用于大型服务器、超级计算机等高性能计算场景 
## 物理组织
**共享内存机器**
- 示例：具有共享内存的多核处理器 。像小型高端单节点计算服务器，通常仅包含几十核心 ，这些核心可以共享内存空间，便于数据交互，提升计算效率。
**分布式内存机器**
- 拥有多个计算节点，每个节点都有自己的本地内存 。这种架构扩展性很强，因为可以方便地添加新节点来提升计算能力。目前所有的超级计算机都属于分布式内存机器，通过众多节点协同工作处理大规模计算任务
**加速器**
-  采用SIMD（Single Instruction Multiple Data，单指令多数据 ）模式进行数据并行计算。例如GPU（图形处理器） ，最初用于图片渲染，因其能并行处理大量数据，现广泛用于深度学习等领域；TPU（张量处理单元）是谷歌专为机器学习设计的定制芯片，针对张量运算进行优化，大幅加速机器学习任务。
# 并行编程模型
- 逻辑组织
- - （或并行计算平台）——提供了一种思考并行程序组织方式的途径
- 基于上述分类，我们有三种并行计算平台：
- - 共享内存
- - 分布式内存
- - 单指令多数据和多线程
# Lab exercise 1
# 内存层次结构
## 程序性能
大多数未优化的并行程序运行时，性能不到机器“峰值”性能的10%。单处理器上也存在大量性能损失，且大部分性能损失来自内存系统
## 硬件与编译器管理
缓存，寄存器和指令级并行由硬件和编译器管理。他们有时能做到最优处理，但也有时做不到。因此，需要编写程序让硬件和编译器更明确如何更好地优化代码以实现高性能
## 内存组件构成层次结构
计算机内存系统包含寄存器
缓存
主存和辅助存储
## 层次结构特性
- 越往上，延迟越低，带宽越高，容量越小
- 越往下，延迟越高，宽带越低，容量越大
## 程序局部性原理
- 空间局部性：访问与之前访问位置临近的内容。
- 时间局部性：重复使用之前访问过的内容。
## 各级缓存功能
